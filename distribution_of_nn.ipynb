{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Donwload a resnet 18 from torch visin\n",
    "import torchvision\n",
    "\n",
    "model = torchvision.models.quantization.resnet18(pretrained=True )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Display the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display the fc layer weight distribution with seaborn\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "print(model.fc.weight.detach().numpy().flatten()[:25])\n",
    "sns.displot(model.fc.weight.detach().numpy().flatten() ,  kind=\"ecdf\")\n",
    "sns.displot( data = model.fc.weight.detach().numpy().flatten() ,  \n",
    "            kind=\"kde\",\n",
    "            rug = False,\n",
    "            )\n",
    "mean = model.fc.weight.detach().numpy().flatten().mean()\n",
    "plt.title(f\"Distribution of fc layer weights the mean is {mean} and the std is {model.fc.weight.detach().numpy().flatten().std():.3f}\")\n",
    "## Add a straight line at the mean\n",
    "plt.axvline(model.fc.weight.detach().numpy().flatten().mean(), color='r')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lovely_tensors as lt\n",
    "lt.monkey_patch()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc.weight.detach().plt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc.weight.detach().flatten().plt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apply torch.fx qat on resnet model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import all the Torch Quantization FX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Apply torch.fx qauntization aware traing resnet 18 model\n",
    "from torch.fx import symbolic_trace\n",
    "from torch.fx.graph_module import GraphModule\n",
    "from torch.ao.quantization.quantize_fx import prepare_qat_fx, convert_fx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Prepare QAT FX \n",
    "1. Fusion\n",
    "2. Model with QAT Modules \n",
    "3. Inserted Observers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a shallow copy of model with copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "fp32_model = copy.copy(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Called the prepare_qat_fx from this PyTorch\n",
    "- https://github.com/pytorch/pytorch/blob/6cef59487ab2e8a7124924f9a76cab381c638ac5/torch/ao/quantization/qconfig_mapping.py#L121\n",
    "- https://github.com/pytorch/pytorch/tree/master/torch/ao/quantization/fx#12-qat-module-swap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "fp32_model.train()\n",
    "## Creat the Qconfig Mapping\n",
    "qconfig_mapping = torch.ao.quantization.qconfig_mapping.get_default_qat_qconfig_mapping(\"fbgemm\")\n",
    "print(qconfig_mapping)\n",
    "## Called the prepare_qat_fx function\n",
    "model = prepare_qat_fx(fp32_model, qconfig_mapping , example_inputs=torch.randn(1, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model)\n",
    "assert isinstance(model, GraphModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print( model.print_readable() ) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Called the Convert method in QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.qconfig_mapping import get_default_qat_qconfig_mapping\n",
    "print( qconfig_mapping.to_dict() )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.ao.quantization.fx.convert import convert\n",
    "model_int8 = convert_fx(\n",
    "    graph_module = model,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from enum import Enum\n",
    "class BackendType(Enum):\n",
    "    Academic = 'Academic'\n",
    "    Tensorrt = 'Tensorrt'\n",
    "    SNPE = 'SNPE'\n",
    "    PPLW8A16 = 'PPLW8A16'\n",
    "    NNIE = 'NNIE'\n",
    "    Vitis = 'Vitis'\n",
    "    ONNX_QNN = 'ONNX_QNN'\n",
    "    PPLCUDA = 'PPLCUDA'\n",
    "    OPENVINO = 'OPENVINO'\n",
    "    Tengine_u8 = \"Tengine_u8\"\n",
    "    Tensorrt_NLP = \"Tensorrt_NLP\"\n",
    "    Academic_NLP = \"Academic_NLP\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rich import print\n",
    "deploy_backend = BackendType.Academic\n",
    "model_mode = 'Training' if model.training else 'Eval'\n",
    "print(\"Quantize model Scheme: {} Mode: {}\".format(deploy_backend, model_mode))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selec the Quantization Scheme of the model\n",
    "- What is the Quantizatin of Activation \n",
    "- What is the Quantization of Weight\n",
    "```\n",
    "    BackendType.Tensorrt:   dict(qtype='affine',     # noqa: E241\n",
    "                                 w_qscheme=QuantizeScheme(symmetry=True, per_channel=True, pot_scale=False, bit=8, symmetric_range=True),\n",
    "                                 a_qscheme=QuantizeScheme(symmetry=True, per_channel=False, pot_scale=False, bit=8, symmetric_range=True),\n",
    "                                 default_weight_quantize=LearnableFakeQuantize,\n",
    "                                 default_act_quantize=LearnableFakeQuantize,\n",
    "                                 default_weight_observer=MinMaxObserver,\n",
    "                                 default_act_observer=EMAMinMaxObserver),\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Qconfig\n",
    "extra_qconfig_dict = prepare_custom_config_dict.get('extra_qconfig_dict', {})\n",
    "qconfig = get_qconfig_by_platform(deploy_backend, extra_qconfig_dict)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cleanrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b8210207dea593cd78a47d19ab578efef95523f0b438f1eecba204cd30d51d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
