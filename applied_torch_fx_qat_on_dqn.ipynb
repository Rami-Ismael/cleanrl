{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import PyTorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "from torch import nn\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, env,\n",
    "                 quantize:bool = False,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Conv2d(4, 32, 8, stride=4),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(32, 64, 4, stride=2),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(64, 64, 3, stride=1),\n",
    "            nn.ReLU(),\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(3136, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, env.single_action_space.n),\n",
    "        )\n",
    "        logging.info(f\"QNetwork: {self.network}\")\n",
    "        ## quantization \n",
    "    def forward(self, x):\n",
    "        return self.network(x / 255.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "import gym\n",
    "from distutils.util import strtobool\n",
    "\n",
    "def parse_args():\n",
    "    # fmt: off\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--seed\", type=int, default=1,\n",
    "        help=\"seed of the experiment\")\n",
    "    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n",
    "    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, cuda will be enabled by default\")\n",
    "    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n",
    "    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n",
    "        help=\"the wandb's project name\")\n",
    "    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n",
    "        help=\"the entity (team) of wandb's project\")\n",
    "    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    parser.add_argument(\"--env-id\", type=str, default=\"BreakoutNoFrameskip-v4\",\n",
    "        help=\"the id of the environment\")\n",
    "    parser.add_argument(\"--total-timesteps\", type=int, default=10000000,\n",
    "        help=\"total timesteps of the experiments\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=1e-4,\n",
    "        help=\"the learning rate of the optimizer\")\n",
    "    parser.add_argument(\"--buffer-size\", type=int, default=1000000,\n",
    "        help=\"the replay memory buffer size\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
    "        help=\"the discount factor gamma\")\n",
    "    parser.add_argument(\"--target-network-frequency\", type=int, default=1000,\n",
    "        help=\"the timesteps it takes to update the target network\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=32,\n",
    "        help=\"the batch size of sample from the reply memory\")\n",
    "    parser.add_argument(\"--start-e\", type=float, default=1,\n",
    "        help=\"the starting epsilon for exploration\")\n",
    "    parser.add_argument(\"--end-e\", type=float, default=0.01,\n",
    "        help=\"the ending epsilon for exploration\")\n",
    "    parser.add_argument(\"--exploration-fraction\", type=float, default=0.10,\n",
    "        help=\"the fraction of `total-timesteps` it takes from start-e to go end-e\")\n",
    "    parser.add_argument(\"--learning-starts\", type=int, default=80000,\n",
    "        help=\"timestep to start learning\")\n",
    "    parser.add_argument(\"--train-frequency\", type=int, default=4,\n",
    "        help=\"the frequency of training\")\n",
    "    \n",
    "    # Quantization\n",
    "    parser.add_argument(\"--quantize\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True)\n",
    "    args = parser.parse_args()\n",
    "    # fmt: on\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:5: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  tensorboard.__version__\n"
     ]
    }
   ],
   "source": [
    "from stable_baselines3.common.atari_wrappers import (\n",
    "    ClipRewardEnv,\n",
    "    EpisodicLifeEnv,\n",
    "    FireResetEnv,\n",
    "    MaxAndSkipEnv,\n",
    "    NoopResetEnv,\n",
    ")\n",
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env = NoopResetEnv(env, noop_max=30)\n",
    "        env = MaxAndSkipEnv(env, skip=4)\n",
    "        env = EpisodicLifeEnv(env)\n",
    "        if \"FIRE\" in env.unwrapped.get_action_meanings():\n",
    "            env = FireResetEnv(env)\n",
    "        env = ClipRewardEnv(env)\n",
    "        env = gym.wrappers.ResizeObservation(env, (84, 84))\n",
    "        env = gym.wrappers.GrayScaleObservation(env)\n",
    "        env = gym.wrappers.FrameStack(env, 4)\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# env setup\n",
    "import gym\n",
    "#args = parse_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "A.L.E: Arcade Learning Environment (version 0.8.0+919230b)\n",
      "[Powered by Stella]\n"
     ]
    }
   ],
   "source": [
    "envs = gym.vector.SyncVectorEnv([make_env(\"BreakoutNoFrameskip-v4\", 42, 0, False, \"run_name\")])\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\"\n",
    "\n",
    "q_network = QNetwork(envs , quantize = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QNetwork</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>network<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sequential</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv2d</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">kernel_size</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">stride</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">))</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv2d</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #808000; text-decoration-color: #808000\">kernel_size</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">stride</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">))</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv2d</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #808000; text-decoration-color: #808000\">kernel_size</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">stride</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">))</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">5</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Flatten</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">start_dim</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">end_dim</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3136</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQNetwork\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnetwork\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m64\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mFlatten\u001b[0m\u001b[1m(\u001b[0m\u001b[33mstart_dim\u001b[0m=\u001b[1;36m1\u001b[0m, \u001b[33mend_dim\u001b[0m=\u001b[1;36m-1\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m3136\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "print(q_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Applied Torch QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span><span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m84\u001b[0m, \u001b[1;36m84\u001b[0m\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "observation_space = envs.single_observation_space.shape\n",
    "print(observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/torch/ao/quantization/observer.py:216: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  reduce_range will be deprecated in a future release of PyTorch.\"\n"
     ]
    }
   ],
   "source": [
    "from torch.ao.quantization.quantize_fx import prepare_fx, convert_fx\n",
    "from torch.ao.quantization import get_default_qat_qconfig_mapping\n",
    "qconfig_mapping = get_default_qat_qconfig_mapping(\"fbgemm\")\n",
    "prepare_qat_q_netork = prepare_fx(q_network , qconfig_mapping=qconfig_mapping  , example_inputs = observation_space)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">torch.ao.quantization.qconfig_mapping.QConfigMapping</span><span style=\"color: #000000; text-decoration-color: #000000\"> object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe89f539c50</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m<\u001b[0m\u001b[1;95mtorch.ao.quantization.qconfig_mapping.QConfigMapping\u001b[0m\u001b[39m object at \u001b[0m\u001b[1;36m0x7fe89f539c50\u001b[0m\u001b[1m>\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">{</span>\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">''</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "    <span style=\"font-weight: bold\">)</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'object_type'</span>: <span style=\"font-weight: bold\">[</span>\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'reshape'</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.ReuseInputObserver'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.NoopObserver'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.conv.Conv1d'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.conv.Conv2d'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.conv.Conv3d'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.conv.ConvTranspose1d'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.conv.ConvTranspose2d'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.conv.ConvTranspose3d'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.linear.Linear'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">built-in</span><span style=\"color: #000000; text-decoration-color: #000000\"> method conv1d of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8df2e8480</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">built-in</span><span style=\"color: #000000; text-decoration-color: #000000\"> method conv2d of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8df2e8480</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">built-in</span><span style=\"color: #000000; text-decoration-color: #000000\"> method conv3d of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8df2e8480</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">built-in</span><span style=\"color: #000000; text-decoration-color: #000000\"> method conv_transpose1d of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8df2e8480</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">built-in</span><span style=\"color: #000000; text-decoration-color: #000000\"> method conv_transpose2d of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8df2e8480</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">built-in</span><span style=\"color: #000000; text-decoration-color: #000000\"> method conv_transpose3d of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8df2e8480</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">built-in</span><span style=\"color: #000000; text-decoration-color: #000000\"> function linear</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.activation.ReLU'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">function</span><span style=\"color: #000000; text-decoration-color: #000000\"> relu at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8c3d80830</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">built-in</span><span style=\"color: #000000; text-decoration-color: #000000\"> method relu of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8df2e8480</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.batchnorm.BatchNorm1d'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.batchnorm.BatchNorm2d'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.batchnorm.BatchNorm3d'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">function</span><span style=\"color: #000000; text-decoration-color: #000000\"> layer_norm at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8c3d828c0</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.PlaceholderObserver'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.normalization.LayerNorm'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.PlaceholderObserver'</span><span style=\"font-weight: bold\">&gt;</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.activation.Hardsigmoid'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00390625</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">function</span><span style=\"color: #000000; text-decoration-color: #000000\"> hardsigmoid at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8c3d82290</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00390625</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'hardsigmoid'</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00390625</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'hardsigmoid_'</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00390625</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.activation.Sigmoid'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00390625</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">built-in</span><span style=\"color: #000000; text-decoration-color: #000000\"> method sigmoid of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8df2e8480</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00390625</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'sigmoid'</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00390625</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'sigmoid_'</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00390625</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.activation.Softmax'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.00390625</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.nn.modules.activation.Tanh'</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0078125</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">built-in</span><span style=\"color: #000000; text-decoration-color: #000000\"> method tanh of type object at </span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0x7fe8df2e8480</span><span style=\"font-weight: bold\">&gt;</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0078125</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'tanh'</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0078125</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>,\n",
       "        <span style=\"font-weight: bold\">(</span>\n",
       "            <span style=\"color: #008000; text-decoration-color: #008000\">'tanh_'</span>,\n",
       "            <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span>\n",
       "<span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.FixedQParamsObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0.0078125</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "                <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}</span>\n",
       "            <span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'module_name_regex'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'module_name'</span>: <span style=\"font-weight: bold\">[]</span>,\n",
       "    <span style=\"color: #008000; text-decoration-color: #008000\">'module_name_object_type_order'</span>: <span style=\"font-weight: bold\">[]</span>\n",
       "<span style=\"font-weight: bold\">}</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m{\u001b[0m\n",
       "    \u001b[32m''\u001b[0m: \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \n",
       "\u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "        \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \n",
       "\u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "    \u001b[1m)\u001b[0m,\n",
       "    \u001b[32m'object_type'\u001b[0m: \u001b[1m[\u001b[0m\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[32m'reshape'\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.ReuseInputObserver'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.NoopObserver'\u001b[0m\u001b[1m>\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.conv.Conv1d'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.conv.Conv2d'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.conv.Conv3d'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.conv.ConvTranspose1d'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.conv.ConvTranspose2d'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.conv.ConvTranspose3d'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.linear.Linear'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mbuilt-in\u001b[0m\u001b[39m method conv1d of type object at \u001b[0m\u001b[1;36m0x7fe8df2e8480\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mbuilt-in\u001b[0m\u001b[39m method conv2d of type object at \u001b[0m\u001b[1;36m0x7fe8df2e8480\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mbuilt-in\u001b[0m\u001b[39m method conv3d of type object at \u001b[0m\u001b[1;36m0x7fe8df2e8480\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mbuilt-in\u001b[0m\u001b[39m method conv_transpose1d of type object at \u001b[0m\u001b[1;36m0x7fe8df2e8480\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mbuilt-in\u001b[0m\u001b[39m method conv_transpose2d of type object at \u001b[0m\u001b[1;36m0x7fe8df2e8480\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mbuilt-in\u001b[0m\u001b[39m method conv_transpose3d of type object at \u001b[0m\u001b[1;36m0x7fe8df2e8480\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mbuilt-in\u001b[0m\u001b[39m function linear\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.activation.ReLU'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mfunction\u001b[0m\u001b[39m relu at \u001b[0m\u001b[1;36m0x7fe8c3d80830\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mbuilt-in\u001b[0m\u001b[39m method relu of type object at \u001b[0m\u001b[1;36m0x7fe8df2e8480\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.batchnorm.BatchNorm1d'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.batchnorm.BatchNorm2d'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.batchnorm.BatchNorm3d'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mfunction\u001b[0m\u001b[39m layer_norm at \u001b[0m\u001b[1;36m0x7fe8c3d828c0\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.PlaceholderObserver'\u001b[0m\u001b[1m>\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.normalization.LayerNorm'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.PlaceholderObserver'\u001b[0m\u001b[1m>\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.activation.Hardsigmoid'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.00390625\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mfunction\u001b[0m\u001b[39m hardsigmoid at \u001b[0m\u001b[1;36m0x7fe8c3d82290\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.00390625\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[32m'hardsigmoid'\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.00390625\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[32m'hardsigmoid_'\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.00390625\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.activation.Sigmoid'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.00390625\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mbuilt-in\u001b[0m\u001b[39m method sigmoid of type object at \u001b[0m\u001b[1;36m0x7fe8df2e8480\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.00390625\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[32m'sigmoid'\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.00390625\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[32m'sigmoid_'\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.00390625\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.activation.Softmax'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.00390625\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.nn.modules.activation.Tanh'\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0078125\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m128\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[1m<\u001b[0m\u001b[1;95mbuilt-in\u001b[0m\u001b[39m method tanh of type object at \u001b[0m\u001b[1;36m0x7fe8df2e8480\u001b[0m\u001b[1m>\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0078125\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m128\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[32m'tanh'\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0078125\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m128\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m,\n",
       "        \u001b[1m(\u001b[0m\n",
       "            \u001b[32m'tanh_'\u001b[0m,\n",
       "            \u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "                \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.fake_quantize.FixedQParamsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\n",
       "\u001b[32m'torch.ao.quantization.observer.FixedQParamsObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;36m0\u001b[0m\u001b[1;36m.0078125\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;36m128\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "                \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "            \u001b[1m)\u001b[0m\n",
       "        \u001b[1m)\u001b[0m\n",
       "    \u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'module_name_regex'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'module_name'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m,\n",
       "    \u001b[32m'module_name_object_type_order'\u001b[0m: \u001b[1m[\u001b[0m\u001b[1m]\u001b[0m\n",
       "\u001b[1m}\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print( qconfig_mapping )\n",
    "example =  qconfig_mapping.to_dict()\n",
    "print(example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.pretty import pprint\n",
    "pprint(torch.ao.quantization.get_default_qat_qconfig('fbgemm'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">GraphModule</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>activation_post_process_0<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FusedMovingAvgObsFakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_affine, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MovingAverageMinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>network<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Module</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConvReLU2d</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv2d</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #808000; text-decoration-color: #808000\">kernel_size</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">8</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">stride</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">))</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConvReLU2d</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv2d</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">32</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #808000; text-decoration-color: #808000\">kernel_size</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">stride</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">))</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ConvReLU2d</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Conv2d</span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">64</span>, <span style=\"color: #808000; text-decoration-color: #808000\">kernel_size</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">stride</span>=<span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">))</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">6</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Flatten</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">start_dim</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">end_dim</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">7</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LinearReLU</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3136</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">9</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">512</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>activation_post_process_1<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FusedMovingAvgObsFakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_affine, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MovingAverageMinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>activation_post_process_2<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FusedMovingAvgObsFakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_affine, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MovingAverageMinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>activation_post_process_3<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FusedMovingAvgObsFakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_affine, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MovingAverageMinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>activation_post_process_4<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FusedMovingAvgObsFakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_affine, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MovingAverageMinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>activation_post_process_5<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FusedMovingAvgObsFakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_affine, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MovingAverageMinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>activation_post_process_6<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FusedMovingAvgObsFakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_affine, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MovingAverageMinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mGraphModule\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mactivation_post_process_0\u001b[1m)\u001b[0m: \u001b[1;35mFusedMovingAvgObsFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_affine, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMovingAverageMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mnetwork\u001b[1m)\u001b[0m: \u001b[1;35mModule\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConvReLU2d\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m32\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m8\u001b[0m, \u001b[1;36m8\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConvReLU2d\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m32\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m, \u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m, \u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConvReLU2d\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mConv2d\u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m64\u001b[0m, \u001b[1;36m64\u001b[0m, \u001b[33mkernel_size\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m, \u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mstride\u001b[0m=\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m, \u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m6\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mFlatten\u001b[0m\u001b[1m(\u001b[0m\u001b[33mstart_dim\u001b[0m=\u001b[1;36m1\u001b[0m, \u001b[33mend_dim\u001b[0m=\u001b[1;36m-1\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m7\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinearReLU\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m3136\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m512\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mactivation_post_process_1\u001b[1m)\u001b[0m: \u001b[1;35mFusedMovingAvgObsFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_affine, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMovingAverageMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mactivation_post_process_2\u001b[1m)\u001b[0m: \u001b[1;35mFusedMovingAvgObsFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_affine, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMovingAverageMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mactivation_post_process_3\u001b[1m)\u001b[0m: \u001b[1;35mFusedMovingAvgObsFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_affine, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMovingAverageMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mactivation_post_process_4\u001b[1m)\u001b[0m: \u001b[1;35mFusedMovingAvgObsFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_affine, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMovingAverageMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mactivation_post_process_5\u001b[1m)\u001b[0m: \u001b[1;35mFusedMovingAvgObsFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_affine, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMovingAverageMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mactivation_post_process_6\u001b[1m)\u001b[0m: \u001b[1;35mFusedMovingAvgObsFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_affine, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMovingAverageMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich.pretty import pprint\n",
    "pprint( prepare_qat_q_netork ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "class GraphModule(torch.nn.Module):\n",
      "    def forward(self, x):\n",
      "        \n",
      "        # File: /tmp/ipykernel_142013/1800980637.py:23, code: return self.network(x / 255.0)\n",
      "        truediv = x / 255.0;  x = None\n",
      "        \n",
      "        # No stacktrace found for following nodes \n",
      "        activation_post_process_0 = self.activation_post_process_0(truediv);  truediv = None\n",
      "        \n",
      "        # File: /home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/torch/ao/quantization/fx/tracer.py:103, code: return super().call_module(m, forward, args, kwargs)\n",
      "        network_0 = getattr(self.network, \"0\")(activation_post_process_0);  activation_post_process_0 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes \n",
      "        activation_post_process_1 = self.activation_post_process_1(network_0);  network_0 = None\n",
      "        \n",
      "        # File: /home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/torch/ao/quantization/fx/tracer.py:103, code: return super().call_module(m, forward, args, kwargs)\n",
      "        network_2 = getattr(self.network, \"2\")(activation_post_process_1);  activation_post_process_1 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes \n",
      "        activation_post_process_2 = self.activation_post_process_2(network_2);  network_2 = None\n",
      "        \n",
      "        # File: /home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/torch/ao/quantization/fx/tracer.py:103, code: return super().call_module(m, forward, args, kwargs)\n",
      "        network_4 = getattr(self.network, \"4\")(activation_post_process_2);  activation_post_process_2 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes \n",
      "        activation_post_process_3 = self.activation_post_process_3(network_4);  network_4 = None\n",
      "        \n",
      "        # File: /home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/torch/ao/quantization/fx/tracer.py:103, code: return super().call_module(m, forward, args, kwargs)\n",
      "        network_6 = getattr(self.network, \"6\")(activation_post_process_3);  activation_post_process_3 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes \n",
      "        activation_post_process_4 = self.activation_post_process_4(network_6);  network_6 = None\n",
      "        \n",
      "        # File: /home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/torch/ao/quantization/fx/tracer.py:103, code: return super().call_module(m, forward, args, kwargs)\n",
      "        network_7 = getattr(self.network, \"7\")(activation_post_process_4);  activation_post_process_4 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes \n",
      "        activation_post_process_5 = self.activation_post_process_5(network_7);  network_7 = None\n",
      "        \n",
      "        # File: /home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/torch/ao/quantization/fx/tracer.py:103, code: return super().call_module(m, forward, args, kwargs)\n",
      "        network_9 = getattr(self.network, \"9\")(activation_post_process_5);  activation_post_process_5 = None\n",
      "        \n",
      "        # No stacktrace found for following nodes \n",
      "        activation_post_process_6 = self.activation_post_process_6(network_9);  network_9 = None\n",
      "        return activation_post_process_6\n",
      "        \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-style: italic\">None</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[3;35mNone\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print( prepare_qat_q_netork.print_readable() ) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cleanrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b8210207dea593cd78a47d19ab578efef95523f0b438f1eecba204cd30d51d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
