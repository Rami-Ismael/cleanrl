{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import Everything"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/torch/utils/tensorboard/__init__.py:5: DeprecationWarning: distutils Version classes are deprecated. Use packaging.version instead.\n",
      "  tensorboard.__version__\n"
     ]
    }
   ],
   "source": [
    "# docs and experiment results can be found at https://docs.cleanrl.dev/rl-algorithms/dqn/#dqnpy\n",
    "import argparse\n",
    "from cmath import log\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from distutils.util import strtobool\n",
    "import logging\n",
    "from typing import Any, Callable, Dict, List, Optional, Tuple, Union\n",
    "from quantize_methods import get_eager_quantization\n",
    "\n",
    "\n",
    "import gym\n",
    "from algos.opt import Adan, hAdam\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from stable_baselines3.common.buffers import ReplayBuffer\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "from torch.ao.quantization.fake_quantize import default_fused_wt_fake_quant , default_weight_fake_quant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import Logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "logging.basicConfig(filename=\"tests.log\", level=logging.NOTSET,\n",
    "                    filemode='w',\n",
    "                    format='%(asctime)s:%(levelname)s:%(filename)s:%(lineno)d:%(message)s')\n",
    "try:\n",
    "    from quantize_methods import size_of_model\n",
    "except ModuleNotFoundError as e:\n",
    "    logging.error(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_args():\n",
    "    # fmt: off\n",
    "    parser = argparse.ArgumentParser()\n",
    "    ## Jupyter Notebook Arguments\n",
    "    parser.add_argument(\"--seed\", type=int, default=1,\n",
    "        help=\"seed of the experiment\")\n",
    "    parser.add_argument(\"--torch-deterministic\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, `torch.backends.cudnn.deterministic=False`\")\n",
    "    parser.add_argument(\"--cuda\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, cuda will be enabled by default\")\n",
    "    parser.add_argument(\"--track\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"if toggled, this experiment will be tracked with Weights and Biases\")\n",
    "    parser.add_argument(\"--wandb-project-name\", type=str, default=\"cleanRL\",\n",
    "        help=\"the wandb's project name\")\n",
    "    parser.add_argument(\"--wandb-entity\", type=str, default=None,\n",
    "        help=\"the entity (team) of wandb's project\")\n",
    "    parser.add_argument(\"--capture-video\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True,\n",
    "        help=\"weather to capture videos of the agent performances (check out `videos` folder)\")\n",
    "\n",
    "    # Algorithm specific arguments\n",
    "    parser.add_argument(\"--env-id\", type=str, default=\"CartPole-v1\",\n",
    "        help=\"the id of the environment\")\n",
    "    parser.add_argument(\"--total-timesteps\", type=int, default=500000,\n",
    "        help=\"total timesteps of the experiments\")\n",
    "    parser.add_argument(\"--learning-rate\", type=float, default=2.5e-4,\n",
    "        help=\"the learning rate of the optimizer\")\n",
    "    parser.add_argument(\"--buffer-size\", type=int, default=10000,\n",
    "        help=\"the replay memory buffer size\")\n",
    "    parser.add_argument(\"--gamma\", type=float, default=0.99,\n",
    "        help=\"the discount factor gamma\")\n",
    "    parser.add_argument(\"--target-network-frequency\", type=int, default=500,\n",
    "        help=\"the timesteps it takes to update the target network\")\n",
    "    parser.add_argument(\"--batch-size\", type=int, default=128,\n",
    "        help=\"the batch size of sample from the reply memory\")\n",
    "    parser.add_argument(\"--start-e\", type=float, default=1,\n",
    "        help=\"the starting epsilon for exploration\")\n",
    "    parser.add_argument(\"--end-e\", type=float, default=0.05,\n",
    "        help=\"the ending epsilon for exploration\")\n",
    "    parser.add_argument(\"--exploration-fraction\", type=float, default=0.5,\n",
    "        help=\"the fraction of `total-timesteps` it takes from start-e to go end-e\")\n",
    "    parser.add_argument(\"--learning-starts\", type=int, default=10000,\n",
    "        help=\"timestep to start learning\")\n",
    "    parser.add_argument(\"--train-frequency\", type=int, default=10,\n",
    "        help=\"the frequency of training\")\n",
    "    \n",
    "    # Quantization specific arguments\n",
    "    ## Quantize Weight\n",
    "    parser.add_argument(\"--quantize-weight\", type=lambda x: bool(strtobool(x)), default=True, nargs=\"?\", const=True)\n",
    "    parser.add_argument(\"--quantize-weight-bitwidth\", type=int, default=8)\n",
    "    parser.add_argument(\"--quantize-weight-quantize-min\", type=int, default= 0)\n",
    "    parser.add_argument(\"--quantize-weight-quantize-max\", type=int, default= 255)\n",
    "    parser.add_argument(\"--quantize-weight-dtype\", type=str, default=\"quint8\")\n",
    "    parser.add_argument(\"--quantize-weight-qschme\", type=str, default=\"per_tensor_symmetric\")\n",
    "    parser.add_argument(\"--quantize-weight-reduce-range\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True)\n",
    "    ## Quantize Activation\n",
    "    parser.add_argument(\"--quantize-activation\" , type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True)\n",
    "    parser.add_argument(\"--quantize-activation-bitwidth\", type=int, default=8)\n",
    "    parser.add_argument(\"--quantize-activation-quantize-min\", type=int, default= 0)\n",
    "    parser.add_argument(\"--quantize-activation-quantize-max\", type=int, default= 255)\n",
    "    parser.add_argument(\"--quantize-activation-qscheme\", type=str, default=\"per_tensor_symmetric\")\n",
    "    parser.add_argument(\"--quantize-activation-quantize-dtype\", type=str, default=\"quint8\")\n",
    "    parser.add_argument(\"--quantize-activation-reduce-range\", type=lambda x: bool(strtobool(x)), default=False, nargs=\"?\", const=True)\n",
    "    ## Other papers algorithm and ideas\n",
    "    parser.add_argument(\"--optimizer\" , type=str, default=\"Adam\")\n",
    "    args = parser.parse_args()\n",
    "    # fmt: on\n",
    "    return args"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_env(env_id, seed, idx, capture_video, run_name):\n",
    "    def thunk():\n",
    "        env = gym.make(env_id)\n",
    "        env = gym.wrappers.RecordEpisodeStatistics(env)\n",
    "        if capture_video:\n",
    "            if idx == 0:\n",
    "                env = gym.wrappers.RecordVideo(env, f\"videos/{run_name}\")\n",
    "        env.seed(seed)\n",
    "        env.action_space.seed(seed)\n",
    "        env.observation_space.seed(seed)\n",
    "        return env\n",
    "\n",
    "    return thunk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, \n",
    "                 env ,\n",
    "                 ):\n",
    "        super().__init__()\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(np.array(env.single_observation_space.shape).prod(), 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, env.single_action_space.n),\n",
    "        )\n",
    "        self.quantize_modules = torch.ao.quantization.QuantStub()\n",
    "        self.dequantize_modules = torch.ao.quantization.DeQuantStub()\n",
    "        logging.info(f\"The model is {self.network} GB\")  \n",
    "        logging.info(f\"The size of the model is {size_of_model(self.network)}\")\n",
    "    def forward(self, x , quantize = False):\n",
    "        return  self.quantize_modules(self.quantize_modules(self.network(x))) if quantize else self.network(x)\n",
    "    ## Fuse the model\n",
    "    def fuse_model(self):\n",
    "        layers = list()\n",
    "        for index in range( 0, len(self.network) - 2 , 2):\n",
    "            layers.append([str(index) , str(index + 1)])\n",
    "        logging.info(f\"Layers to fuse {layers}\")\n",
    "        print(f\"Layers to fuse {layers}\")\n",
    "        torch.ao.quantization.fuse_modules(self.network, layers, inplace=True)\n",
    "    def get_quantization_config(self):\n",
    "        if self.quantize_weight:\n",
    "            if self.quantize_activation:\n",
    "                return torch.ao.quantization.QConfig(\n",
    "                    activation = torch.ao.quantization.FakeQuantize.with_args(\n",
    "                        observer = torch.ao.quantization.MovingAverageMinMaxObserver(\n",
    "                            dtype = self.quantize_activation_quantize_dtype,\n",
    "                            reduce_range = self.quantize_activation_quantize_reduce_range,\n",
    "                            quant_min = self.quantize_activation_quantize_min,\n",
    "                            quant_max = self.quantize_activation_quantize_max,\n",
    "                        )\n",
    "                    ),\n",
    "                    weight = torch.ao.quantization.FakeQuantize.with_args(\n",
    "                        observer = torch.ao.quantization.MovingAverageMinMaxObserver(\n",
    "                            dtype = torch.quint8,\n",
    "                            quant_min = -128,\n",
    "                            quant_max = 127,\n",
    "                        )\n",
    "                    )\n",
    "                )\n",
    "            else:\n",
    "                return torch.ao.quantization.QConfig(\n",
    "                    activation = torch.nn.Identity,\n",
    "                    weight = default_weight_fake_quant,\n",
    "                )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linear Schedule"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_schedule(start_e: float, end_e: float, duration: int, t: int):\n",
    "    slope = (end_e - start_e) / duration\n",
    "    return max(slope * t + start_e, end_e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parse Arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = dict()\n",
    "args[\"env\"] = \"CartPole-v0\"\n",
    "args[\"quantize_activation_quantize_dtype\"] = torch.quint8\n",
    "args[\"seed\"] = 0\n",
    "args[\"torch_deterministic\"] = True\n",
    "args[\"env_id\"] = \"CartPole-v0\"\n",
    "args[\"capture_video\"] = False\n",
    "## Quantization\n",
    "args[\"quantize_weight\"] = True\n",
    "args[\"quantize_weight_bitwidth\"] = 8\n",
    "args[\"quantize_observation_type\"] = \"moving_average_min_max\"\n",
    "args[\"quantize_weight_quantize_min\"] = 0\n",
    "args[\"quantize_weight_quantize_max\"] = 255\n",
    "args[\"quantize_weight_dtype\"] = \"quint8\"\n",
    "args[\"quantize_weight_qscheme\"] = \"per_tensor_symmetric\"\n",
    "args[\"quantize_weight_reduce_range\"] = False\n",
    "## Quantize Activation\n",
    "args[\"quantize_activation\"] = True\n",
    "args[\"quantize-activation-bitwidth\"] = 8\n",
    "args[\"quantize_observation_type\"] = \"moving_average_min_max\"\n",
    "args[\"quantize_activation_quantize_min\"] = 0\n",
    "args[\"quantize_activation_quantize_max\"] = 255\n",
    "args[\"quantize_activation_dtype\"] = \"quint8\"\n",
    "args[\"quantize_activation_qscheme\"] = \"per_tensor_symmetric\"\n",
    "args[\"quantize_activation_reduce_range\"] = False\n",
    "\n",
    "## Convert the dictionary to a namespace\n",
    "args = argparse.Namespace(**args)\n",
    "assert args.quantize_activation_dtype == \"quint8\", f\"The activation dtype must be quint8 and {args.quantize_activation_dtype}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified the argument where the string pytorch quantization datatype will be PyTorch Quantization Datatype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">quint8\n",
       "</pre>\n"
      ],
      "text/plain": [
       "quint8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">torch.quint8\n",
       "</pre>\n"
      ],
      "text/plain": [
       "torch.quint8\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "if args.quantize_activation_quantize_dtype is not None and isinstance(args.quantize_activation_quantize_dtype, str):\n",
    "    if args.quantize_activation_quantize_dtype == \"quint8\":\n",
    "        args.quantize_activation_quantize_dtype = torch.quint8\n",
    "    elif args.quantize_activation_quantize_dtype == \"qint8\":\n",
    "        args.quantize_activation_quantize_dtype = torch.qint8\n",
    "    else:\n",
    "        print(args.quantize_activation_quantize_dtype)\n",
    "        raise ValueError(f\"{args.quantize_activation_quantize_dtype} is not supported for quantization\")\n",
    "if args.quantize_weight_dtype is not None and isinstance(args.quantize_weight_dtype, str):\n",
    "    if args.quantize_weight_dtype == \"quint8\":\n",
    "        args.quantize_weight_dtype = torch.quint8\n",
    "    elif args.quantize_weight_dtype == \"qint8\":\n",
    "        args.quantize_weight_dtype = torch.qint8\n",
    "    else:\n",
    "        raise ValueError(f\"{args.quantize_weight_dtype} is not supported for quantization\")\n",
    "assert isinstance(args.quantize_activation_quantize_dtype, torch.dtype), f\"The activation dtype must be torch.dtype and {type(args.quantize_activation_quantize_dtype)}\"\n",
    "print(args.quantize_activation_dtype)\n",
    "print(args.quantize_weight_dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modified the argument where the string pytorch quantization Quantization Scheme "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "if args.quantize_activation_qscheme is not None and isinstance( args.quantize_activation_qscheme , str):\n",
    "    if args.quantize_activation_qscheme == \"per_tensor_symmetric\":\n",
    "        args.quantize_activation_qscheme = torch.per_tensor_symmetric\n",
    "    elif args.quantize_activation_qscheme == \"per_tensor_affine\":\n",
    "        args.quantize_activation_qscheme = torch.per_tensor_affine\n",
    "    else:\n",
    "        raise ValueError(f\"{args.quantize_activation_qscheme} is not supported for quantization\")\n",
    "if args.quantize_weight_qscheme is not None and isinstance(args.quantize_weight_qscheme, str):\n",
    "    if args.quantize_weight_qscheme == \"per_tensor_symmetric\":\n",
    "        args.quantize_weight_qscheme = torch.per_tensor_symmetric\n",
    "    elif args.quantize_weight_qscheme == \"per_tensor_affine\":\n",
    "        args.quantize_weight_qscheme = torch.per_tensor_affine\n",
    "    else:\n",
    "        raise ValueError(f\"{args.quantize_weight_qscheme} is not supported for quantization\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRY NOT TO MODIFY: seeding\n",
    "random.seed(args.seed)\n",
    "np.random.seed(args.seed)\n",
    "torch.manual_seed(args.seed)\n",
    "torch.backends.cudnn.deterministic = args.torch_deterministic"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set the Devices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() and args.cuda else \"cpu\")\n",
    "logging.info(f\"The device the DQN is running on: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/gym/envs/registration.py:506: UserWarning: \u001b[33mWARN: The environment CartPole-v0 is out of date. You should consider upgrading to version `v1` with the environment ID `CartPole-v1`.\u001b[0m\n",
      "  f\"The environment {path} is out of date. You should consider \"\n",
      "/home/null/miniconda3/envs/cleanrl/lib/python3.7/site-packages/gym/core.py:173: DeprecationWarning: \u001b[33mWARN: Function `env.seed(seed)` is marked as deprecated and will be removed in the future. Please use `env.reset(seed=seed) instead.\u001b[0m\n",
      "  \"Function `env.seed(seed)` is marked as deprecated and will be removed in the future. \"\n"
     ]
    }
   ],
   "source": [
    "run_name = f\"{args.env}_{args.seed}_{int(time.time())}\"\n",
    "# env setup\n",
    "envs = gym.vector.SyncVectorEnv([make_env(args.env_id, args.seed, 0, args.capture_video, run_name)])\n",
    "assert isinstance(envs.single_action_space, gym.spaces.Discrete), \"only discrete action space is supported\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Build the Neural Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QNetwork</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>network<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sequential</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>quantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuantStub</span><span style=\"font-weight: bold\">()</span>\n",
       "  <span style=\"font-weight: bold\">(</span>dequantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DeQuantStub</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQNetwork\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnetwork\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mquantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mQuantStub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdequantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mDeQuantStub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QNetwork</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>network<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sequential</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>quantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuantStub</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>dequantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DeQuantStub</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQNetwork\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mnetwork\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mquantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mQuantStub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mdequantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mDeQuantStub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from rich import print\n",
    "q_network = QNetwork(\n",
    "                    env = envs,\n",
    "                        )\n",
    "print(q_network)\n",
    "from rich.pretty import pprint\n",
    "pprint(q_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Eager Quantization to the method fuse the modules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. To quantize the model for inference in using eager quantization, we need to fuse the model in where the model is eval mode\n",
    "2. Called the fuse method\n",
    "3. Set the Quantization Configuration for the model\n",
    "4. Called the prepare QAT mode "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Layers to fuse <span style=\"font-weight: bold\">[[</span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Layers to fuse \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'0'\u001b[0m, \u001b[32m'1'\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[32m'2'\u001b[0m, \u001b[32m'3'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QNetwork</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>network<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sequential</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LinearReLU</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Identity</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LinearReLU</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Identity</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>quantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuantStub</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>dequantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DeQuantStub</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQNetwork\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mnetwork\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinearReLU\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mIdentity\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinearReLU\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mIdentity\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mquantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mQuantStub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mdequantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mDeQuantStub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_network.eval()\n",
    "q_network.fuse_model()\n",
    "pprint(q_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appled the Q Config to the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example of Q Config from fbgemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \n",
       "\u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \n",
       "\u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAverageMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">){}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"font-weight: bold\">&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.qint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_channel_symmetric<span style=\"font-weight: bold\">){}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAverageMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mreduce_range\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FusedMovingAvgObsFakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MovingAveragePerChannelMinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.qint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_channel_symmetric\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "## fbgemm\n",
    "from torch.ao.quantization import get_default_qat_qconfig\n",
    "\n",
    "example_qconfig = get_default_qat_qconfig(\"fbgemm\")\n",
    "print(example_qconfig)\n",
    "pprint(example_qconfig)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Eager Quantization Cofiguration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "from torch.ao.quantization.fake_quantize import FakeQuantize\n",
    "from torch.ao.quantization import MinMaxObserver\n",
    "from torch.ao.quantization.qconfig import QConfig\n",
    "import torch\n",
    "def get_eager_quantization(\n",
    "    weight_quantize:bool  = True,\n",
    "    weight_observer_type:str = \"moving_average_minmax\",\n",
    "    weight_quantization_min:int = 0,\n",
    "    weight_quantization_max:int = 255,\n",
    "    weight_quantization_dtype:torch.dtype = torch.quint8,\n",
    "    weight_quantization_qscheme:torch.qscheme = torch.per_tensor_symmetric,\n",
    "    weight_reduce_range = True,\n",
    "    activation_quantize:bool = True,\n",
    "    activation_observer_type:str = \"moving_average_minmax\",\n",
    "    activation_quantization_min:int = -128,\n",
    "    activation_quantization_max:int = 127,\n",
    "    activation_quantization_dtype:torch.dtype = torch.quint8,\n",
    "    activation_quantization_qscheme:torch.qscheme = torch.per_tensor_symmetric,\n",
    "    *args, **kwargs\n",
    "):\n",
    "    assert isinstance( weight_quantization_dtype , torch.dtype)\n",
    "    assert isinstance( activation_quantization_dtype , torch.dtype)\n",
    "    assert isinstance( weight_quantization_qscheme , torch.qscheme)\n",
    "    assert isinstance( activation_quantization_qscheme , torch.qscheme)\n",
    "    ## all quantization  in eager mode are unifrom quantization \n",
    "    weight_quantization_fake_quantize = None\n",
    "    if weight_quantize:\n",
    "        weight_quantization_fake_quantize = FakeQuantize.with_args(\n",
    "                    observer =  MinMaxObserver.with_args(\n",
    "                        dtype = weight_quantization_dtype,\n",
    "                        qscheme = weight_quantization_qscheme,\n",
    "                        reduce_range = False , \n",
    "                        quant_min= weight_quantization_min,\n",
    "                        quant_max = weight_quantization_max,\n",
    "                    ))\n",
    "    activation_quantization_fake_quantize = None\n",
    "    if activation_quantize:\n",
    "            activation_quantization_fake_quantize = FakeQuantize.with_args(\n",
    "                    observer =   MinMaxObserver.with_args( \n",
    "                        quant_min = activation_quantization_min,\n",
    "                        quant_max = activation_quantization_max,\n",
    "                        dtype = activation_quantization_dtype,\n",
    "                        qscheme = activation_quantization_qscheme,\n",
    "                        reduce_range = False\n",
    "                    ))\n",
    "    return QConfig(\n",
    "        weight = weight_quantization_fake_quantize,\n",
    "        activation = activation_quantization_fake_quantize\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the Default Q Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "\u001b[2;32m│   \u001b[0m\u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QConfig</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">activation</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span><span style=\"font-weight: bold\">){}){}</span>,\n",
       "    <span style=\"color: #808000; text-decoration-color: #808000\">weight</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.fake_quantize.FakeQuantize'</span><span style=\"font-weight: bold\">&gt;</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">observer</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">functools</span><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">.partial</span><span style=\"font-weight: bold\">(&lt;</span><span style=\"color: #ff00ff; text-decoration-color: #ff00ff; font-weight: bold\">class</span><span style=\"color: #000000; text-decoration-color: #000000\"> </span><span style=\"color: #008000; text-decoration-color: #008000\">'torch.ao.quantization.observer.MinMaxObserver'</span><span style=\"font-weight: bold\">&gt;</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">reduce_range</span>=<span style=\"color: #ff0000; text-decoration-color: #ff0000; font-style: italic\">False</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span><span style=\"font-weight: bold\">){}){}</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQConfig\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[33mactivation\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \n",
       "\u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m,\n",
       "    \u001b[33mweight\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.fake_quantize.FakeQuantize'\u001b[0m\u001b[1m>\u001b[0m, \n",
       "\u001b[33mobserver\u001b[0m=\u001b[1;35mfunctools\u001b[0m\u001b[1;35m.partial\u001b[0m\u001b[1m(\u001b[0m\u001b[1m<\u001b[0m\u001b[1;95mclass\u001b[0m\u001b[39m \u001b[0m\u001b[32m'torch.ao.quantization.observer.MinMaxObserver'\u001b[0m\u001b[1m>\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \n",
       "\u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mreduce_range\u001b[0m=\u001b[3;91mFalse\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\u001b[1m)\u001b[0m\u001b[1m{\u001b[0m\u001b[1m}\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "q_config = get_eager_quantization()\n",
    "pprint(q_config)\n",
    "print(q_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set the Q Configuration of the Q Netork "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_network.train()\n",
    "q_network.qconfig = q_config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the model for QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (network): Sequential(\n",
       "    (0): LinearReLU(\n",
       "      in_features=4, out_features=120, bias=True\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (1): Identity()\n",
       "    (2): LinearReLU(\n",
       "      in_features=120, out_features=84, bias=True\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (3): Identity()\n",
       "    (4): Linear(\n",
       "      in_features=84, out_features=2, bias=True\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quantize_modules): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=-128, quant_max=127, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequantize_modules): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.ao.quantization.prepare_qat(q_network, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QNetwork</span><span style=\"font-weight: bold\">(</span>\n",
       "  <span style=\"font-weight: bold\">(</span>network<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sequential</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LinearReLU</span><span style=\"font-weight: bold\">(</span>\n",
       "      <span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "      <span style=\"font-weight: bold\">(</span>weight_fake_quant<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>,\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Identity</span><span style=\"font-weight: bold\">()</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LinearReLU</span><span style=\"font-weight: bold\">(</span>\n",
       "      <span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "      <span style=\"font-weight: bold\">(</span>weight_fake_quant<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>,\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Identity</span><span style=\"font-weight: bold\">()</span>\n",
       "    <span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span>\n",
       "      <span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "      <span style=\"font-weight: bold\">(</span>weight_fake_quant<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>,\n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "        <span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "        <span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>quantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuantStub</span><span style=\"font-weight: bold\">(</span>\n",
       "    <span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "      <span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, \n",
       "<span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "      <span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "    <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">)</span>\n",
       "  <span style=\"font-weight: bold\">(</span>dequantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DeQuantStub</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQNetwork\u001b[0m\u001b[1m(\u001b[0m\n",
       "  \u001b[1m(\u001b[0mnetwork\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinearReLU\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "      \u001b[1m(\u001b[0mweight_fake_quant\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \n",
       "\u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mIdentity\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinearReLU\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[33min_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "      \u001b[1m(\u001b[0mweight_fake_quant\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \n",
       "\u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mIdentity\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "    \u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[33min_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "      \u001b[1m(\u001b[0mweight_fake_quant\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m,\n",
       "\u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "        \u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \n",
       "\u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "        \u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "      \u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mquantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mQuantStub\u001b[0m\u001b[1m(\u001b[0m\n",
       "    \u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "      \u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \n",
       "\u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \n",
       "\u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "      \u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "    \u001b[1m)\u001b[0m\n",
       "  \u001b[1m)\u001b[0m\n",
       "  \u001b[1m(\u001b[0mdequantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mDeQuantStub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QNetwork</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>network<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sequential</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LinearReLU</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span>weight_fake_quant<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Identity</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LinearReLU</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span>weight_fake_quant<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Identity</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span>weight_fake_quant<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">255</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   │   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>quantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuantStub</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">FakeQuantize</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"color: #808000; text-decoration-color: #808000\">fake_quant_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">observer_enabled</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.uint8<span style=\"font-weight: bold\">)</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_min</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-128</span>, <span style=\"color: #808000; text-decoration-color: #808000\">quant_max</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">127</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.quint8, <span style=\"color: #808000; text-decoration-color: #808000\">qscheme</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.per_tensor_symmetric, <span style=\"color: #808000; text-decoration-color: #808000\">ch_axis</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">-1</span>, <span style=\"color: #808000; text-decoration-color: #808000\">scale</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span>.<span style=\"font-weight: bold\">])</span>, <span style=\"color: #808000; text-decoration-color: #808000\">zero_point</span>=<span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">tensor</span><span style=\"font-weight: bold\">([</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">]</span>, <span style=\"color: #808000; text-decoration-color: #808000\">dtype</span>=<span style=\"color: #800080; text-decoration-color: #800080\">torch</span>.int32<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span>activation_post_process<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">MinMaxObserver</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">min_val</span>=<span style=\"color: #800080; text-decoration-color: #800080\">inf</span>, <span style=\"color: #808000; text-decoration-color: #808000\">max_val</span>=-inf<span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>dequantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DeQuantStub</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQNetwork\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mnetwork\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinearReLU\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0mweight_fake_quant\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mIdentity\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinearReLU\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0mweight_fake_quant\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mIdentity\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0mweight_fake_quant\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m0\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m255\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   │   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mquantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mQuantStub\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mFakeQuantize\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[33mfake_quant_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mobserver_enabled\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.uint8\u001b[1m)\u001b[0m, \u001b[33mquant_min\u001b[0m=\u001b[1;36m-128\u001b[0m, \u001b[33mquant_max\u001b[0m=\u001b[1;36m127\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.quint8, \u001b[33mqscheme\u001b[0m=\u001b[35mtorch\u001b[0m.per_tensor_symmetric, \u001b[33mch_axis\u001b[0m=\u001b[1;36m-1\u001b[0m, \u001b[33mscale\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m1\u001b[0m.\u001b[1m]\u001b[0m\u001b[1m)\u001b[0m, \u001b[33mzero_point\u001b[0m=\u001b[1;35mtensor\u001b[0m\u001b[1m(\u001b[0m\u001b[1m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m]\u001b[0m, \u001b[33mdtype\u001b[0m=\u001b[35mtorch\u001b[0m.int32\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0mactivation_post_process\u001b[1m)\u001b[0m: \u001b[1;35mMinMaxObserver\u001b[0m\u001b[1m(\u001b[0m\u001b[33mmin_val\u001b[0m=\u001b[35minf\u001b[0m, \u001b[33mmax_val\u001b[0m=-inf\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mdequantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mDeQuantStub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(q_network)\n",
    "pprint(q_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applied Use Spefic Q Config Pattern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">Layers to fuse <span style=\"font-weight: bold\">[[</span><span style=\"color: #008000; text-decoration-color: #008000\">'0'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'1'</span><span style=\"font-weight: bold\">]</span>, <span style=\"font-weight: bold\">[</span><span style=\"color: #008000; text-decoration-color: #008000\">'2'</span>, <span style=\"color: #008000; text-decoration-color: #008000\">'3'</span><span style=\"font-weight: bold\">]]</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "Layers to fuse \u001b[1m[\u001b[0m\u001b[1m[\u001b[0m\u001b[32m'0'\u001b[0m, \u001b[32m'1'\u001b[0m\u001b[1m]\u001b[0m, \u001b[1m[\u001b[0m\u001b[32m'2'\u001b[0m, \u001b[32m'3'\u001b[0m\u001b[1m]\u001b[0m\u001b[1m]\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QNetwork</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>network<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Sequential</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LinearReLU</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Identity</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">LinearReLU</span><span style=\"font-weight: bold\">(</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">0</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">120</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│     </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">1</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">ReLU</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">3</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Identity</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">│   </span><span style=\"font-weight: bold\">(</span><span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">4</span><span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">Linear</span><span style=\"font-weight: bold\">(</span><span style=\"color: #808000; text-decoration-color: #808000\">in_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">84</span>, <span style=\"color: #808000; text-decoration-color: #808000\">out_features</span>=<span style=\"color: #008080; text-decoration-color: #008080; font-weight: bold\">2</span>, <span style=\"color: #808000; text-decoration-color: #808000\">bias</span>=<span style=\"color: #00ff00; text-decoration-color: #00ff00; font-style: italic\">True</span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">)</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>quantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">QuantStub</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"color: #7fbf7f; text-decoration-color: #7fbf7f\">  </span><span style=\"font-weight: bold\">(</span>dequantize_modules<span style=\"font-weight: bold\">)</span>: <span style=\"color: #800080; text-decoration-color: #800080; font-weight: bold\">DeQuantStub</span><span style=\"font-weight: bold\">()</span>\n",
       "<span style=\"font-weight: bold\">)</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1;35mQNetwork\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mnetwork\u001b[1m)\u001b[0m: \u001b[1;35mSequential\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinearReLU\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m4\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mIdentity\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinearReLU\u001b[0m\u001b[1m(\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m120\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│     \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mReLU\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mIdentity\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m│   \u001b[0m\u001b[1m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1m)\u001b[0m: \u001b[1;35mLinear\u001b[0m\u001b[1m(\u001b[0m\u001b[33min_features\u001b[0m=\u001b[1;36m84\u001b[0m, \u001b[33mout_features\u001b[0m=\u001b[1;36m2\u001b[0m, \u001b[33mbias\u001b[0m=\u001b[3;92mTrue\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mquantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mQuantStub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[2;32m  \u001b[0m\u001b[1m(\u001b[0mdequantize_modules\u001b[1m)\u001b[0m: \u001b[1;35mDeQuantStub\u001b[0m\u001b[1m(\u001b[0m\u001b[1m)\u001b[0m\n",
       "\u001b[1m)\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "QNetwork(\n",
       "  (network): Sequential(\n",
       "    (0): LinearReLU(\n",
       "      in_features=4, out_features=120, bias=True\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (1): Identity()\n",
       "    (2): LinearReLU(\n",
       "      in_features=120, out_features=84, bias=True\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "    (3): Identity()\n",
       "    (4): Linear(\n",
       "      in_features=84, out_features=2, bias=True\n",
       "      (weight_fake_quant): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "      (activation_post_process): FakeQuantize(\n",
       "        fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "        (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (quantize_modules): QuantStub(\n",
       "    (activation_post_process): FakeQuantize(\n",
       "      fake_quant_enabled=tensor([1], dtype=torch.uint8), observer_enabled=tensor([1], dtype=torch.uint8), quant_min=0, quant_max=255, dtype=torch.quint8, qscheme=torch.per_tensor_symmetric, ch_axis=-1, scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32)\n",
       "      (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequantize_modules): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "1. Create another Q Network\n",
    "2. Fuse the modules\n",
    "3. Set the Quantization Config\n",
    "4. called prepare for QAT\n",
    "'''\n",
    "q_network = QNetwork(\n",
    "    env=envs)\n",
    "## set the model to eval mode\n",
    "q_network.eval()\n",
    "## fuse the modules\n",
    "q_network.fuse_model()\n",
    "pprint(q_network)\n",
    "## set the model to trian \n",
    "q_network.train()\n",
    "## set the user spefici quantization config\n",
    "q_network.qconfig = get_eager_quantization(\n",
    "            weight_quantize = args.quantize_weight,\n",
    "            weight_observer_type = \"moving_average_min_max\",\n",
    "            weight_quantization_min =  args.quantize_weight_quantize_min , \n",
    "            weight_quantization_max = args.quantize_weight_quantize_max,\n",
    "            weight_quantization_dtype = args.quantize_weight_dtype,\n",
    "            weight_reduce_range= args.quantize_weight_reduce_range,\n",
    "            activation_quantize= args.quantize_activation,\n",
    "            activation_quantization_min = args.quantize_activation_quantize_min,\n",
    "            activation_quantization_max = args.quantize_activation_quantize_max,\n",
    "            activation_quantization_dtype = args.quantize_activation_quantize_dtype,\n",
    "            activation_quantization_qscheme = args.quantize_activation_qscheme,\n",
    "            activation_reduce_range = args.quantize_activation_reduce_range,\n",
    ")\n",
    "torch.ao.quantization.prepare_qat(q_network, inplace=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('cleanrl')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "3b8210207dea593cd78a47d19ab578efef95523f0b438f1eecba204cd30d51d3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
